# -*- coding: utf-8 -*-
"""seth-nw-med-scaper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cwmKexGNAZIR3kYIdHZjjjHN9glCx2mC
"""


# Credit: reference code MP 2.1 Year 2021 from the teaching team of CS 410 

# installations and updates
!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
!pip install selenium

# imports
import math
from time import sleep
from bs4 import BeautifulSoup
import re 
import time
import urllib
from selenium import webdriver
import requests
from csv import writer
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# instantiate webdriver options
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

# instantiate driver
driver = webdriver.Chrome('chromedriver',options=options)

#uses webdriver object to execute javascript code and get dynamically loaded webcontent
def get_js_soup(url,driver):
    driver.get(url)
    res_html = driver.execute_script('return document.body.innerHTML')
    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content
    return soup

## tidies extracted text 
#def process_bio(bio):
#    bio = bio.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters
#    bio = re.sub('\s+',' ',bio)       #repalces repeated whitespace characters with single space
#    return bio

''' More tidying
Sometimes the text extracted HTML webpage may contain javascript code and some style elements. 
This function removes script and style tags from HTML so that extracted text does not contain them.
'''
def remove_script(soup):
    for script in soup(["script", "style"]):
        script.decompose()
    return soup


#Checks if bio_url is a valid faculty homepage
def is_valid_homepage(bio_url,dir_url):
    if bio_url.endswith('.pdf'): #we're not parsing pdfs
        return False
    try:
        #sometimes the homepage url points to the same page as the faculty profile page
        #which should be treated differently from an actual homepage
        ret_url = urllib.request.urlopen(bio_url).geturl() 
    except:
        return False       #unable to access bio_url
    urls = [re.sub('((https?://)|(www.))','',url) for url in [ret_url,dir_url]] #removes url scheme (https,http or www) 
    return not(urls[0]== urls[1])

#extracts all Faculty Profile page urls from the Directory Listing Page
def scrape_dir_page(dir_url,driver):
    print ('-'*20,'Scraping directory page','-'*20)
    faculty_links = []
    #faculty_base_url = 'https://cookcountyhealth.org/about/physicians-directory/'
    #execute js on webpage to load faculty listings on webpage and get ready to parse the loaded HTML 
    soup = get_js_soup(dir_url,driver)  
    for link_holder in soup.find_all(class_='card-flex'): #get list of all of class 'name'
        rel_link = link_holder.find('a')['href'] #get url
        #print(rel_link)
        #url returned is relative, so we need to add base url
        faculty_links.append(rel_link) 
    print ('-'*20,'Found {} faculty profile urls'.format(len(faculty_links)),'-'*20)
    return faculty_links

# parameters (set these)
RESULTS_PER_PAGE = 96 # must be one of (24, 48, 96) -- 96 is the max allowed per NM query
MAX_RESULTS = 6070 # parameter - set this accordingly
BASE_TARGET_URL = 'https://www.nm.org/doctors/search-results?latitude=41.8946401&longitude=-87.6211275&sort=distance&page='

# calculated parameters (these are set automatically)
num_pages = math.ceil(MAX_RESULTS / RESULTS_PER_PAGE)
pg = 0 # loop will increment before scrape function call

# conduct scraping in loop
results = []
for pg in range(num_pages):
  target_url = BASE_TARGET_URL + str(pg + 1) + "&pagesize=" + str(RESULTS_PER_PAGE)
  results += scrape_dir_page(target_url, driver)
  sleep(1) # pause to not exceed timeout

# clean the results
clean_results = []
print("Length of original results: ", len(results))
for i in range(len(results)):
  if 'foundation.nm.org' not in results[i]:
    clean_results.append(results[i])

print("Length of cleaned results: ", len(clean_results))

# code to save cleaned list of hyperlinks using pickle
import pickle

from google.colab import drive
drive.mount('/content/drive')

#with open('drive/My Drive/nw-med-hyperlinks.pickle', 'wb') as f:
#  pickle.dump(clean_results, f)
#  print("successful")

# import list of clean hyperlinks from pickle and store in "nw_med_links"
from google.colab import drive
import pickle
drive.mount('/content/drive')

with open('drive/My Drive/nw-med-hyperlinks.pkl', 'rb') as fin:
  nw_med_links = pickle.load(fin)
  print("imported - first five links:")
  print('\n'.join(nw_med_links[:5]))
  print("len of nw_med_links: ", len(nw_med_links))

# loop through each link 
import requests
for link in nw_med_links:
    
    # multiple sessions tries to avoid http connections errors https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url-in-requests/47475019#47475019
    session = requests.Session()
    retry = Retry(connect=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    
    page = session.get(link)


    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find_all(class_='section-doctor-profile')

    with open('doctors.csv', 'a', newline='', encoding='utf8') as f:
        thewriter = writer(f)
        header = ['Name', 'Specialty', 'Location', 'Education', 'Residency', 'Board Certification']
        thewriter.writerow(header)
    
        for result in results: 
            out = result.find('img')['alt']
            name = out.rsplit(', ', 1)[0]
            print(name)
            try: 
                specialty = result.find_all('h3')[0].a.text
                specialty = "".join(specialty.rstrip())
                print(specialty)
            except AttributeError:
                specialty = "nil"
                print(specialty)   
            try: 
                location = result.find(text='On Medical Staff At:').findNext('div').text
                print(location)
            except AttributeError:
                location = "nil"
                print(location)    
            try: 
                education = result.find(text='Medical Education:').findNext('div').text
                education = "".join(education.rstrip())
                print(education)
            except AttributeError:
                education = "nil"
                print(education)
            try: 
                residency = result.find(text='Residency:').findNext('div').text
                residency = "".join(residency.rstrip())
                print(residency)
            except AttributeError:
                residency = "nil"
                print(residency)
            try: 
                certification = result.find_all('li').find(text='certified').findNext('li').text
                #certification = "".join(certification.rstrip())
                print(certification)
            except AttributeError:
                certification = "nil"
                print(certification)
            doctorinfo = [name, specialty, location, education, residency, certification]
            thewriter.writerow(doctorinfo)
            print()